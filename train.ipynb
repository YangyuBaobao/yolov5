{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "FILE = Path(\"train.py\").resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "import val  # for end-of-epoch mAP\n",
    "from models.experimental import attempt_load\n",
    "from models.yolo import Model\n",
    "from utils.autoanchor import check_anchors\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\n",
    "    strip_optimizer, get_latest_run, check_dataset, check_git_status, check_img_size, check_requirements, \\\n",
    "    check_file, check_yaml, check_suffix, print_args, print_mutation, set_logging, one_cycle, colorstr, methods\n",
    "from utils.downloads import attempt_download\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.plots import plot_labels, plot_evolve\n",
    "from utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, intersect_dicts, select_device, \\\n",
    "    torch_distributed_zero_first\n",
    "from utils.loggers.wandb.wandb_utils import check_wandb_resume\n",
    "from utils.metrics import fitness\n",
    "from utils.loggers import Loggers\n",
    "from utils.callbacks import Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger(__name__)\n",
    "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\n",
    "RANK = int(os.getenv('RANK', -1))\n",
    "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\n",
    "\n",
    "def copyfile(srcfile,dstpath):\n",
    "    if not os.path.isfile(srcfile):\n",
    "        print (\"%s not exist!\"%(srcfile))\n",
    "    else:\n",
    "        shutil.copy(srcfile, dstpath)\n",
    "        print (\"copy %s -> %s\"%(srcfile, dstpath ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyp,  # path/to/hyp.yaml or hyp dictionary\n",
    "          opt,\n",
    "          device,\n",
    "          callbacks\n",
    "          ):\n",
    "    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, = \\\n",
    "        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n",
    "        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n",
    "\n",
    "    # Directories\n",
    "    w = save_dir / 'weights'  # weights dir\n",
    "    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "    copyfile(opt.cfg, w)\n",
    "    last, best = w / 'last.pt', w / 'best.pt'\n",
    "\n",
    "    # Hyperparameters\n",
    "    if isinstance(hyp, str):\n",
    "        with open(hyp, errors='ignore') as f:\n",
    "            hyp = yaml.safe_load(f)  # load hyps dict\n",
    "    LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\n",
    "\n",
    "    # Save run settings\n",
    "    with open(save_dir / 'hyp.yaml', 'w') as f:\n",
    "        yaml.safe_dump(hyp, f, sort_keys=False)\n",
    "    with open(save_dir / 'opt.yaml', 'w') as f:\n",
    "        yaml.safe_dump(vars(opt), f, sort_keys=False)\n",
    "    data_dict = None\n",
    "\n",
    "    # Loggers\n",
    "    if RANK in [-1, 0]:\n",
    "        loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n",
    "        if loggers.wandb:\n",
    "            data_dict = loggers.wandb.data_dict\n",
    "            if resume:\n",
    "                weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp\n",
    "\n",
    "        # Register actions\n",
    "        for k in methods(loggers):\n",
    "            callbacks.register_action(k, callback=getattr(loggers, k))\n",
    "\n",
    "    # Config\n",
    "    plots = not evolve  # create plots\n",
    "    cuda = device.type != 'cpu'\n",
    "    init_seeds(1 + RANK)\n",
    "    with torch_distributed_zero_first(LOCAL_RANK):\n",
    "        data_dict = data_dict or check_dataset(data)  # check if None\n",
    "    train_path, val_path = data_dict['train'], data_dict['val']\n",
    "    nc = 1 if single_cls else int(data_dict['nc'])  # number of classes\n",
    "    names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\n",
    "    assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # check\n",
    "    is_coco = data.endswith('coco.yaml') and nc == 80  # COCO dataset\n",
    "\n",
    "    # Model\n",
    "    # check_suffix(weights, '.pt')  # check weights\n",
    "    pretrained = weights.endswith('.pt')\n",
    "    if pretrained:\n",
    "        with torch_distributed_zero_first(LOCAL_RANK):\n",
    "            weights = attempt_download(weights)  # download if not found locally\n",
    "        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys\n",
    "        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
    "        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n",
    "        model.load_state_dict(csd, strict=False)  # load\n",
    "        LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report\n",
    "    else:\n",
    "        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "\n",
    "    # Freeze\n",
    "    freeze = [f'model.{x}.' for x in range(freeze)]  # layers to freeze\n",
    "    for k, v in model.named_parameters():\n",
    "        v.requires_grad = True  # train all layers\n",
    "        if any(x in k for x in freeze):\n",
    "            print(f'freezing {k}')\n",
    "            v.requires_grad = False\n",
    "\n",
    "    # Optimizer\n",
    "    nbs = 64  # nominal batch size\n",
    "    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n",
    "    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n",
    "    LOGGER.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\n",
    "\n",
    "    g0, g1, g2 = [], [], []  # optimizer parameter groups\n",
    "    for v in model.modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias\n",
    "            g2.append(v.bias)\n",
    "        if isinstance(v, nn.BatchNorm2d):  # weight (no decay)\n",
    "            g0.append(v.weight)\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n",
    "            g1.append(v.weight)\n",
    "\n",
    "    if opt.adam:\n",
    "        optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "    else:\n",
    "        optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "\n",
    "    optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\n",
    "    optimizer.add_param_group({'params': g2})  # add g2 (biases)\n",
    "    LOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups \"\n",
    "                f\"{len(g0)} weight, {len(g1)} weight (no decay), {len(g2)} bias\")\n",
    "    del g0, g1, g2\n",
    "\n",
    "    # Scheduler\n",
    "    if opt.linear_lr:\n",
    "        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear\n",
    "    else:\n",
    "        lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)\n",
    "\n",
    "    # EMA\n",
    "    ema = ModelEMA(model) if RANK in [-1, 0] else None\n",
    "\n",
    "    # Resume\n",
    "    start_epoch, best_fitness = 0, 0.0\n",
    "    if pretrained:\n",
    "        # Optimizer\n",
    "        if ckpt['optimizer'] is not None:\n",
    "            optimizer.load_state_dict(ckpt['optimizer'])\n",
    "            best_fitness = ckpt['best_fitness']\n",
    "\n",
    "        # EMA\n",
    "        if ema and ckpt.get('ema'):\n",
    "            ema.ema.load_state_dict(ckpt['ema'].float().state_dict())\n",
    "            ema.updates = ckpt['updates']\n",
    "\n",
    "        # Epochs\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        if resume:\n",
    "            assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'\n",
    "        if epochs < start_epoch:\n",
    "            LOGGER.info(f\"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.\")\n",
    "            epochs += ckpt['epoch']  # finetune additional epochs\n",
    "\n",
    "        del ckpt, csd\n",
    "\n",
    "    # Image sizes\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])\n",
    "    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n",
    "\n",
    "    # DP mode\n",
    "    if cuda and RANK == -1 and torch.cuda.device_count() > 1:\n",
    "        logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\\n'\n",
    "                        'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # SyncBatchNorm\n",
    "    if opt.sync_bn and cuda and RANK != -1:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
    "        LOGGER.info('Using SyncBatchNorm()')\n",
    "\n",
    "    # Trainloader\n",
    "    train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\n",
    "                                              hyp=hyp, augment=True, cache=opt.cache, rect=opt.rect, rank=LOCAL_RANK,\n",
    "                                              workers=workers, image_weights=opt.image_weights, quad=opt.quad,\n",
    "                                              prefix=colorstr('train: '))\n",
    "    mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class\n",
    "    nb = len(train_loader)  # number of batches\n",
    "    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'\n",
    "\n",
    "    # Process 0\n",
    "    if RANK in [-1, 0]:\n",
    "        val_loader = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,\n",
    "                                       hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1,\n",
    "                                       workers=workers, pad=0.5,\n",
    "                                       prefix=colorstr('val: '))[0]\n",
    "\n",
    "        if not resume:\n",
    "            labels = np.concatenate(dataset.labels, 0)\n",
    "            # c = torch.tensor(labels[:, 0])  # classes\n",
    "            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency\n",
    "            # model._initialize_biases(cf.to(device))\n",
    "            if plots:\n",
    "                plot_labels(labels, names, save_dir)\n",
    "\n",
    "            # Anchors\n",
    "            if not opt.noautoanchor:\n",
    "                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
    "            model.half().float()  # pre-reduce anchor precision\n",
    "\n",
    "        callbacks.run('on_pretrain_routine_end')\n",
    "\n",
    "    # DDP mode\n",
    "    if cuda and RANK != -1:\n",
    "        model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)\n",
    "\n",
    "    # Model parameters\n",
    "    hyp['box'] *= 3. / nl  # scale to layers\n",
    "    hyp['cls'] *= nc / 80. * 3. / nl  # scale to classes and layers\n",
    "    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # scale to image size and layers\n",
    "    hyp['label_smoothing'] = opt.label_smoothing\n",
    "    model.nc = nc  # attach number of classes to model\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n",
    "    model.names = names\n",
    "\n",
    "    # Start training\n",
    "    t0 = time.time()\n",
    "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\n",
    "    last_opt_step = -1\n",
    "    maps = np.zeros(nc)  # mAP per class\n",
    "    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
    "    scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "    scaler = amp.GradScaler(enabled=cuda)\n",
    "    stopper = EarlyStopping(patience=opt.patience)\n",
    "    compute_loss = ComputeLoss(model)  # init loss class\n",
    "    LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\\n'\n",
    "                f'Using {train_loader.num_workers} dataloader workers\\n'\n",
    "                f\"Logging results to {colorstr('bold', save_dir)}\\n\"\n",
    "                f'Starting training for {epochs} epochs...')\n",
    "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "        model.train()\n",
    "\n",
    "        # Update image weights (optional, single-GPU only)\n",
    "        if opt.image_weights:\n",
    "            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights\n",
    "            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\n",
    "            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\n",
    "\n",
    "        # Update mosaic border (optional)\n",
    "        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
    "        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
    "\n",
    "        mloss = torch.zeros(3, device=device)  # mean losses\n",
    "        if RANK != -1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        pbar = enumerate(train_loader)\n",
    "        LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\n",
    "        if RANK in [-1, 0]:\n",
    "            pbar = tqdm(pbar, total=nb)  # progress bar\n",
    "        optimizer.zero_grad()\n",
    "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "            # Warmup\n",
    "            if ni <= nw:\n",
    "                xi = [0, nw]  # x interp\n",
    "                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "            # Multi-scale\n",
    "            if opt.multi_scale:\n",
    "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "                if sf != 1:\n",
    "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                    imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Forward\n",
    "            with amp.autocast(enabled=cuda):\n",
    "                pred = model(imgs)  # forward\n",
    "                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
    "                if RANK != -1:\n",
    "                    loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\n",
    "                if opt.quad:\n",
    "                    loss *= 4.\n",
    "\n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Optimize\n",
    "            if ni - last_opt_step >= accumulate:\n",
    "                scaler.step(optimizer)  # optimizer.step\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if ema:\n",
    "                    ema.update(model)\n",
    "                last_opt_step = ni\n",
    "\n",
    "            # Log\n",
    "            if RANK in [-1, 0]:\n",
    "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\n",
    "                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (\n",
    "                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))\n",
    "                callbacks.run('on_train_batch_end', ni, model, imgs, targets, paths, plots, opt.sync_bn)\n",
    "            # end batch ------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Scheduler\n",
    "        lr = [x['lr'] for x in optimizer.param_groups]  # for loggers\n",
    "        scheduler.step()\n",
    "\n",
    "        if RANK in [-1, 0]:\n",
    "            # mAP\n",
    "            callbacks.run('on_train_epoch_end', epoch=epoch)\n",
    "            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])\n",
    "            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop\n",
    "            if not noval or final_epoch:  # Calculate mAP\n",
    "                results, maps, _ = val.run(data_dict,\n",
    "                                           batch_size=batch_size // WORLD_SIZE * 2,\n",
    "                                           imgsz=imgsz,\n",
    "                                           model=ema.ema,\n",
    "                                           single_cls=single_cls,\n",
    "                                           dataloader=val_loader,\n",
    "                                           save_dir=save_dir,\n",
    "                                           plots=False,\n",
    "                                           callbacks=callbacks,\n",
    "                                           compute_loss=compute_loss)\n",
    "\n",
    "            # Update best mAP\n",
    "            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\n",
    "            if fi > best_fitness:\n",
    "                best_fitness = fi\n",
    "            log_vals = list(mloss) + list(results) + lr\n",
    "            callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)\n",
    "\n",
    "            # Save model\n",
    "            if (not nosave) or (final_epoch and not evolve):  # if save\n",
    "                ckpt = {'epoch': epoch,\n",
    "                        'best_fitness': best_fitness,\n",
    "                        'model': deepcopy(de_parallel(model)).half(),\n",
    "                        'ema': deepcopy(ema.ema).half(),\n",
    "                        'updates': ema.updates,\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'wandb_id': loggers.wandb.wandb_run.id if loggers.wandb else None}\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(ckpt, last)\n",
    "                if best_fitness == fi:\n",
    "                    torch.save(ckpt, best)\n",
    "                if (epoch > 0) and (opt.save_period > 0) and (epoch % opt.save_period == 0):\n",
    "                    torch.save(ckpt, w / f'epoch{epoch}.pt')\n",
    "                del ckpt\n",
    "                callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)\n",
    "\n",
    "            # Stop Single-GPU\n",
    "            if RANK == -1 and stopper(epoch=epoch, fitness=fi):\n",
    "                break\n",
    "\n",
    "            # Stop DDP TODO: known issues shttps://github.com/ultralytics/yolov5/pull/4576\n",
    "            # stop = stopper(epoch=epoch, fitness=fi)\n",
    "            # if RANK == 0:\n",
    "            #    dist.broadcast_object_list([stop], 0)  # broadcast 'stop' to all ranks\n",
    "\n",
    "        # Stop DPP\n",
    "        # with torch_distributed_zero_first(RANK):\n",
    "        # if stop:\n",
    "        #    break  # must break all DDP ranks\n",
    "\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n",
    "    # end training -----------------------------------------------------------------------------------------------------\n",
    "    if RANK in [-1, 0]:\n",
    "        LOGGER.info(f'\\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.')\n",
    "        for f in last, best:\n",
    "            if f.exists():\n",
    "                strip_optimizer(f)  # strip optimizers\n",
    "                if f is best:\n",
    "                    LOGGER.info(f'\\nValidating {f}...')\n",
    "                    results, _, _ = val.run(data_dict,\n",
    "                                            batch_size=batch_size // WORLD_SIZE * 2,\n",
    "                                            imgsz=imgsz,\n",
    "                                            model=attempt_load(f, device).half(),\n",
    "                                            iou_thres=0.65 if is_coco else 0.60,  # best pycocotools results at 0.65\n",
    "                                            single_cls=single_cls,\n",
    "                                            dataloader=val_loader,\n",
    "                                            save_dir=save_dir,\n",
    "                                            save_json=is_coco,\n",
    "                                            verbose=True,\n",
    "                                            plots=True,\n",
    "                                            callbacks=callbacks,\n",
    "                                            compute_loss=compute_loss)  # val best model with plots\n",
    "\n",
    "        callbacks.run('on_train_end', last, best, plots, epoch)\n",
    "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Opt:\n",
    "    weights = ROOT / ''\n",
    "    cfg = ROOT /'models/yolov5s.yaml'\n",
    "    data = ROOT / 'data/myvoc.yaml'\n",
    "    hyp = ROOT / 'data/hyps/hyp.scratch-high.yaml'\n",
    "    project = ROOT / 'runs/train'\n",
    "    epochs = 300\n",
    "    batch_size = 30\n",
    "    imgsz = 640\n",
    "    img = 640\n",
    "    img_size = 640\n",
    "    react = False\n",
    "    device = ''\n",
    "    adam=False\n",
    "    artifact_alias='latest'\n",
    "    bbox_interval=-1\n",
    "    bucket=''\n",
    "    cache=None\n",
    "    entity=None\n",
    "    evolve=None\n",
    "    exist_ok=False\n",
    "    freeze=0\n",
    "    image_weights=False\n",
    "    label_smoothing=0.0\n",
    "    linear_lr=False\n",
    "    local_rank=-1\n",
    "    multi_scale=False\n",
    "    name = \"src_data\"\n",
    "    noautoanchor=False\n",
    "    nosave=False\n",
    "    noval=False\n",
    "    patience=100\n",
    "    quad=False\n",
    "    resume=False\n",
    "    save_period=-1\n",
    "    single_cls=False\n",
    "    upload_dataset=False\n",
    "    workers=8\n",
    "    sync_bn=False\n",
    "\n",
    "\n",
    "def parse_opt(known=False):\n",
    "    opt = Opt()\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(opt, callbacks=Callbacks()):\n",
    "    # Checks\n",
    "    set_logging(RANK)\n",
    "    if RANK in [-1, 0]:\n",
    "        print_args(FILE.stem, opt)\n",
    "        check_git_status()\n",
    "        check_requirements(exclude=['thop'])\n",
    "\n",
    "    # Resume\n",
    "    if opt.resume and not check_wandb_resume(opt) and not opt.evolve:  # resume an interrupted run\n",
    "        ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path\n",
    "        assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'\n",
    "        with open(Path(ckpt).parent.parent / 'opt.yaml', errors='ignore') as f:\n",
    "            opt = argparse.Namespace(**yaml.safe_load(f))  # replace\n",
    "        opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate\n",
    "        LOGGER.info(f'Resuming training from {ckpt}')\n",
    "    else:\n",
    "        opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = \\\n",
    "            check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks\n",
    "        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'\n",
    "        if opt.evolve:\n",
    "            opt.project = str(ROOT / 'runs/evolve')\n",
    "            opt.exist_ok, opt.resume = opt.resume, False  # pass resume to exist_ok and disable resume\n",
    "        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))\n",
    "\n",
    "    # DDP mode\n",
    "    device = select_device(opt.device, batch_size=opt.batch_size)\n",
    "    if LOCAL_RANK != -1:\n",
    "        assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'\n",
    "        assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'\n",
    "        assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'\n",
    "        assert not opt.evolve, '--evolve argument is not compatible with DDP training'\n",
    "        torch.cuda.set_device(LOCAL_RANK)\n",
    "        device = torch.device('cuda', LOCAL_RANK)\n",
    "        dist.init_process_group(backend=\"nccl\" if dist.is_nccl_available() else \"gloo\")\n",
    "\n",
    "    # Train\n",
    "    if not opt.evolve:\n",
    "        train(opt.hyp, opt, device, callbacks)\n",
    "        if WORLD_SIZE > 1 and RANK == 0:\n",
    "            LOGGER.info('Destroying process group... ')\n",
    "            dist.destroy_process_group()\n",
    "\n",
    "    # Evolve hyperparameters (optional)\n",
    "    else:\n",
    "        # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit)\n",
    "        meta = {'lr0': (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "                'lrf': (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "                'momentum': (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1\n",
    "                'weight_decay': (1, 0.0, 0.001),  # optimizer weight decay\n",
    "                'warmup_epochs': (1, 0.0, 5.0),  # warmup epochs (fractions ok)\n",
    "                'warmup_momentum': (1, 0.0, 0.95),  # warmup initial momentum\n",
    "                'warmup_bias_lr': (1, 0.0, 0.2),  # warmup initial bias lr\n",
    "                'box': (1, 0.02, 0.2),  # box loss gain\n",
    "                'cls': (1, 0.2, 4.0),  # cls loss gain\n",
    "                'cls_pw': (1, 0.5, 2.0),  # cls BCELoss positive_weight\n",
    "                'obj': (1, 0.2, 4.0),  # obj loss gain (scale with pixels)\n",
    "                'obj_pw': (1, 0.5, 2.0),  # obj BCELoss positive_weight\n",
    "                'iou_t': (0, 0.1, 0.7),  # IoU training threshold\n",
    "                'anchor_t': (1, 2.0, 8.0),  # anchor-multiple threshold\n",
    "                'anchors': (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)\n",
    "                'fl_gamma': (0, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "                'hsv_h': (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)\n",
    "                'hsv_s': (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)\n",
    "                'hsv_v': (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)\n",
    "                'degrees': (1, 0.0, 45.0),  # image rotation (+/- deg)\n",
    "                'translate': (1, 0.0, 0.9),  # image translation (+/- fraction)\n",
    "                'scale': (1, 0.0, 0.9),  # image scale (+/- gain)\n",
    "                'shear': (1, 0.0, 10.0),  # image shear (+/- deg)\n",
    "                'perspective': (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001\n",
    "                'flipud': (1, 0.0, 1.0),  # image flip up-down (probability)\n",
    "                'fliplr': (0, 0.0, 1.0),  # image flip left-right (probability)\n",
    "                'mosaic': (1, 0.0, 1.0),  # image mixup (probability)\n",
    "                'mixup': (1, 0.0, 1.0),  # image mixup (probability)\n",
    "                'copy_paste': (1, 0.0, 1.0)}  # segment copy-paste (probability)\n",
    "\n",
    "        with open(opt.hyp, errors='ignore') as f:\n",
    "            hyp = yaml.safe_load(f)  # load hyps dict\n",
    "            if 'anchors' not in hyp:  # anchors commented in hyp.yaml\n",
    "                hyp['anchors'] = 3\n",
    "        opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # only val/save final epoch\n",
    "        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices\n",
    "        evolve_yaml, evolve_csv = save_dir / 'hyp_evolve.yaml', save_dir / 'evolve.csv'\n",
    "        if opt.bucket:\n",
    "            os.system(f'gsutil cp gs://{opt.bucket}/evolve.csv {save_dir}')  # download evolve.csv if exists\n",
    "\n",
    "        for _ in range(opt.evolve):  # generations to evolve\n",
    "            if evolve_csv.exists():  # if evolve.csv exists: select best hyps and mutate\n",
    "                # Select parent(s)\n",
    "                parent = 'single'  # parent selection method: 'single' or 'weighted'\n",
    "                x = np.loadtxt(evolve_csv, ndmin=2, delimiter=',', skiprows=1)\n",
    "                n = min(5, len(x))  # number of previous results to consider\n",
    "                x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n",
    "                w = fitness(x) - fitness(x).min() + 1E-6  # weights (sum > 0)\n",
    "                if parent == 'single' or len(x) == 1:\n",
    "                    # x = x[random.randint(0, n - 1)]  # random selection\n",
    "                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n",
    "                elif parent == 'weighted':\n",
    "                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n",
    "\n",
    "                # Mutate\n",
    "                mp, s = 0.8, 0.2  # mutation probability, sigma\n",
    "                npr = np.random\n",
    "                npr.seed(int(time.time()))\n",
    "                g = np.array([meta[k][0] for k in hyp.keys()])  # gains 0-1\n",
    "                ng = len(meta)\n",
    "                v = np.ones(ng)\n",
    "                while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n",
    "                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n",
    "                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n",
    "                    hyp[k] = float(x[i + 7] * v[i])  # mutate\n",
    "\n",
    "            # Constrain to limits\n",
    "            for k, v in meta.items():\n",
    "                hyp[k] = max(hyp[k], v[1])  # lower limit\n",
    "                hyp[k] = min(hyp[k], v[2])  # upper limit\n",
    "                hyp[k] = round(hyp[k], 5)  # significant digits\n",
    "\n",
    "            # Train mutation\n",
    "            results = train(hyp.copy(), opt, device, callbacks)\n",
    "\n",
    "            # Write mutation results\n",
    "            print_mutation(results, hyp.copy(), save_dir, opt.bucket)\n",
    "\n",
    "        # Plot results\n",
    "        plot_evolve(evolve_csv)\n",
    "        print(f'Hyperparameter evolution finished\\n'\n",
    "              f\"Results saved to {colorstr('bold', save_dir)}\\n\"\n",
    "              f'Use best hyperparameters example: $ python train.py --hyp {evolve_yaml}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parse_opt()\n",
    "main(opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inductionNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
